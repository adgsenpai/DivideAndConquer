# Q8. Merge Sort vs Quick Sort (divide-and-conquer)

## (a) Merge Sort recurrence, solution, space, and stability

**Recurrence:**
```
T(n) = 2T(n/2) + Θ(n)    for n > 1
T(1) = Θ(1)              base case
```

**Solution using Master Theorem:**
- a = 2, b = 2, f(n) = Θ(n)
- n^(log_b a) = n^(log_2 2) = n^1 = n
- f(n) = Θ(n) = Θ(n^(log_b a))  → Case 2
- **T(n) = Θ(n log n)**

**Alternative derivation (recursion tree):**
- Depth of tree: log₂ n
- Work at each level: n (merging all elements)
- Total: n × log n = Θ(n log n)

**Space Usage:**
- **Auxiliary space:** Θ(n) for temporary merge array
- **Total space:** Θ(n log n) if all recursive frames counted, but typically we reuse merge buffer
- **Practical:** Θ(n) extra space for merge buffer

**Stability:**
- **YES**, Merge Sort is stable
- During merge, when A[i] = B[j], we take from A first (preserving original order)
- Equal elements maintain their relative order from input

---

## (b) Quick Sort with median-of-three and cutoff (hybrid)

**Pseudocode:**
```
QUICKSORT_HYBRID(A, left, right, t):
    if right - left + 1 <= t:
        INSERTION_SORT(A, left, right)
        return
    
    // Median-of-three pivot selection
    mid = (left + right) / 2
    medianIndex = MEDIAN_OF_THREE(A, left, mid, right)
    swap A[medianIndex] with A[right]  // Move pivot to end
    
    pivotIndex = PARTITION(A, left, right)
    QUICKSORT_HYBRID(A, left, pivotIndex - 1, t)
    QUICKSORT_HYBRID(A, pivotIndex + 1, right, t)

MEDIAN_OF_THREE(A, i, j, k):
    // Returns index of median among A[i], A[j], A[k]
    if A[i] < A[j]:
        if A[j] < A[k]:
            return j
        else if A[i] < A[k]:
            return k
        else:
            return i
    else:
        if A[i] < A[k]:
            return i
        else if A[j] < A[k]:
            return k
        else:
            return j
```

**Why this hybrid wins in practice:**

1. **Median-of-three pivot:**
   - Avoids worst-case on sorted/nearly-sorted data
   - Approximates true median with O(1) overhead
   - Reduces chance of unbalanced partitions

2. **Cutoff to insertion sort (t ≈ 10-20):**
   - Small subarrays: insertion sort has lower constants
   - Avoids recursion overhead for tiny arrays
   - Insertion sort is fast for nearly-sorted small arrays
   - Cache-friendly on small data

3. **In-place:** No auxiliary space like merge sort

4. **Cache locality:** Quick sort has better cache behavior (in-place, sequential access)

---

## (c) Choosing practical cutoff threshold t (RAM model)

**Factors to consider:**

1. **Recursion overhead:**
   - Each recursive call has frame setup/teardown cost
   - For n ≤ t, recursion overhead > sorting benefit
   - Typical overhead: ~10-50 cycles per call

2. **Insertion sort constants:**
   - Insertion sort: ~c₁ × n² operations (c₁ small)
   - Quick sort: ~c₂ × n log n (c₂ larger due to partitioning)
   - Break-even when c₁n² ≈ c₂n log n
   - Solve: n ≈ c₂ log n / c₁

3. **Branch prediction:**
   - Small arrays: fewer branch mispredictions
   - Insertion sort: simple, predictable branches
   - Quick sort: more complex branching logic

4. **Cache effects:**
   - L1 cache line: typically 64 bytes
   - If array fits in cache (e.g., 10-20 integers), insertion sort very fast
   - No cache misses for small t

**Theoretical estimate:**
- Insertion sort: ~c₁n² comparisons
- Quick sort + recursion: ~c₂n log n + c₃ (recursion overhead)
- Break-even: c₁t² ≈ c₂t log t + c₃

**Practical choice:**
- **t = 10-20** works well in most implementations
- Can be tuned via micro-benchmarks for specific architectures
- Some libraries use t ≈ 7-16

**Why not t = 1?**
- Too much recursion overhead
- Every single element requires two recursive calls (though they return immediately)

**Why not t = 100?**
- Insertion sort is O(n²), becomes slow
- 100² = 10,000 operations vs ~100 × log(100) ≈ 664

**Optimal range: t ∈ [7, 20]** based on:
- Empirical measurements
- Balance of recursion cost vs insertion sort work
- Cache line considerations
